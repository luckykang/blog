---
layout: post
title: 机器学习系列18丨决策树算法（二）
tag: 机器学习
---

### 一.剪枝的介绍

#### 1.为什么剪枝？

![20211026144741](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211026144741.png)

如上图所示，我们看到随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降。
出现这种情况的原因是：

- 噪声、样本冲突，即错误的样本数据。

- 特征即属性不能完全作为分类标准。

- 巧合的规律性，数据量不够大。

#### 2.常用的剪枝方法

**预剪枝**

在构建树的过程中，同时剪枝。

比如：

（1）限制每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分；

（2）指定树的高度或者深度，例如树的最大深度为4；

（3）指定结点的熵小于某个值，不再划分。随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降。

**后剪枝**

