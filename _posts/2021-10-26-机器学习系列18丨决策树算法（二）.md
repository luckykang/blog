---
layout: post
title: 机器学习系列18丨决策树算法（二）
tag: 机器学习
---

### 一.剪枝的介绍

#### 1.为什么剪枝？

![20211026144741](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211026144741.png)

如上图所示，我们看到随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降。
出现这种情况的原因是：

- 噪声、样本冲突，即错误的样本数据。

- 特征即属性不能完全作为分类标准。

- 巧合的规律性，数据量不够大。

#### 2.常用的剪枝方法

**预剪枝**

在构建树的过程中，同时剪枝。

比如：

（1）限制每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分；

（2）指定树的高度或者深度，例如树的最大深度为4；

（3）指定结点的熵小于某个值，不再划分。随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降。

**后剪枝**

后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。

### 二.决策树算法API介绍

- class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)

    - **criterion**
        - 特征选择标准
        - "gini"或者"entropy"，前者代表基尼系数，后者代表信息增益。一默认"gini"，即CART算法。

    - **min_samples_split**
        - 内部节点再划分所需最小样本数
        - 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。

    - **min_samples_leaf**
        - 叶子节点最少样本数
        - 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。

    - **max_depth**
        - 决策树最大深度
        - 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间

    - **random_state**
        - 随机数种子

### 三.案例演示

    import pandas as pd
    import numpy as np
    # 数据集划分
    from sklearn.model_selection import train_test_split
    # 特征工程-字典特征提取
    from sklearn.feature_extraction import dict_vectorizer
    # 决策树算法
    from sklearn.tree import DecisionTreeClassifier, export_graphviz

    # 1.获取数据
    titan = pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")

    # 2.数据基本处理
    # 2.1 确定特征值,目标值
    x = titan[["pclass", "age", "sex"]]
    y = titan["survived"]
    # 2.2 缺失值处理
    # 缺失值需要处理，将特征当中有类别的这些特征进行字典特征抽取
    x['age'].fillna(x['age'].mean(), inplace=True)
    # 2.3 数据集划分
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22)

    # 3.特征工程(字典特征抽取)
    # 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)
    # x.to_dict(orient="records") 需要将数组特征转换成字典数据
    transfer = dict_vectorizer(sparse=False)

    x_train = transfer.fit_transform(x_train.to_dict(orient="records"))
    x_test = transfer.fit_transform(x_test.to_dict(orient="records"))

    # 4.决策树模型训练
    # 决策树API当中，如果没有指定max_depth那么会根据信息熵的条件直到最终结束。这里我们可以指定树的深度来进行限制树的大小
    # 4.机器学习(决策树)
    estimator = DecisionTreeClassifier(criterion="entropy", max_depth=5)
    estimator.fit(x_train, y_train)

    # 5.模型评估
    # 预测值
    y_pre = estimator.score(x_test, y_test)
    print(y_pre)
    # 准确率
    ret = estimator.predict(x_test)
    print(ret)

### 四.决策树可视化

保存树的结构到dot文件：

sklearn.tree.export_graphviz() 该函数能够导出DOT格式

    export_graphviz(estimator, out_file="./data/tree.dot", feature_names=['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', '女性', '男性'])

我们可以在打开生成的将dot文件，并把内容复制到该网站，查看结构：

[http://webgraphviz.com/](http://webgraphviz.com/)


![20211027155634](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211027155634.png)

### 五.决策树总结

**优点：**

简单的理解和解释，树木可视化。

**缺点：**

决策树学习者可以创建不能很好地推广数据的过于复杂的树,容易发生过拟合。

**改进：**

  - 剪枝cart算法
  - 随机森林（集成学习的一种）

**注：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多，可以选择特征**