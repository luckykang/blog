---
layout: post
title: 机器学习系列20丨聚类算法
tag: 机器学习
---

### 一.聚类算法简介

#### 1.概念

一种典型的**无监督学习算法**，主要用于将相似的样本自动归到一个类别中。

在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。

使用不同的聚类准则，产生的聚类结果不同。

#### 2.应用

- 用户画像，广告推荐，Data Segmentation，搜索引擎的流量推荐，恶意流量识别

- 基于位置信息的商业推送，新闻聚类，筛选排序

- 图像分割，降维，识别；离群点检测；信用卡异常消费；发掘相同功能的基因片段

#### 3.聚类算法与分类算法最大的区别

聚类算法是**无监督**的学习算法，而分类算法属于**监督**的学习算法。

### 二.聚类算法API的使用

#### 1.api介绍

- sklearn.cluster.KMeans(n_clusters=8)

    - 参数:
        - n_clusters:开始的聚类中心数量
        - 整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。
    - 方法:
        - estimator.fit(x)
        - estimator.predict(x)
        - estimator.fit_predict(x)
        - 计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)

#### 2.使用

    import matplotlib.pyplot as plt
    from sklearn.datasets.samples_generator import make_blobs
    # 使用k-means进行聚类
    from sklearn.cluster import KMeans
    # CH方法评估
    from sklearn.metrics import calinski_harabaz_score

    # 获取数据
    # X为样本特征，Y为样本簇类别， 共1000个样本，每个样本4个特征，共4个簇，
    # 有多少样本 n_samples
    # 有几个特征 n_features
    # 簇中心在[-1,-1], [0,0],[1,1], [2,2]
    # 簇方差分别为[0.4, 0.2, 0.2, 0.2]
    # 随机数种子 random_state
    X, Y = make_blobs(n_samples=1000, n_features=2, centers=[[-1, -1], [0, 0], [1, 1], [2, 2]],
                    cluster_std=[0.4, 0.2, 0.2, 0.2],
                    random_state=9)

    # 模型训练
    # n_clusters表示聚类中心数量
    estimator = KMeans(n_clusters=5, random_state=2)
    y_pre = estimator.fit_predict(X)

    # 模型可视化
    plt.scatter(X[:, 0], X[:, 1], c=y_pre)
    plt.show()

    # CH方法评估聚类分数
    print(calinski_harabaz_score(X, y_pre))

结果:

![20211028175647](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211028175647.png)

### 三.聚类算法实现流程

#### 1.k-means其实包含两层内容:

**K:** 初始中心点个数（计划聚类数）

**​means:** 求中心点到其他数据点距离的平均值

#### 2.k-means聚类步骤

- 事先确定常数K，常数K意味着最终的聚类类别数;

- 首先随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，

- 接着，重新计算每个类的质心(即为类中心)，重复这样的过程，直到质心不再改变

- 最终就确定了每个样本所属的类别以及每个类的质心。

![20211029150355](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029150355.png)

#### 3.小结

kmeans由于要计算质心到每一个样本的距离，所以其收敛速度比较慢。

### 四.模型评估

#### 1.误差平方和(SSE \The sum of squares due to error)

![20211029163215](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029163215.png)

SSE图最终的结果,对图松散度的衡量.(eg: SSE(左图)<SSE(右图)

**SSE随着聚类迭代,其值会越来越小,直到最后趋于稳定**

![20211029163642](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029163642.png)

如果质心的初始值选择不好,SSE只会达到一个不怎么好的局部最优解

![20211029163755](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029163755.png)

#### 2.“肘”方法 (Elbow method) — K值确定

对于n个点的数据集，迭代计算k from 1 to n，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和

平方和是会逐渐变小的，直到k==n时平方和为0，因为每个点都是它所在的簇中心本身

在这个平方和变化过程中，会出现一个拐点也即“肘”点，**下降率突然变缓时即认为是最佳的k值**，所以在拐点选取K值比较合适。

决定什么时候停止训练呢？在增加分类无法带来更多回报时，我们停止增加类别。

![20211029164109](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029164109.png)

#### 3.SC轮廓系数法（Silhouette Coefficient）

结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。​取值为[-1, 1]，S的值越大越好

![20211029164936](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029164936.png)

**目的：**

内部距离最小化（a越小越好），外部距离最大化（b越大越好）

#### 4.CH系数（Calinski-Harabasz Index）

类别内部数据的协方差越小越好，类别之间的协方差越大越好（换句话说：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好）。

**tr 为矩阵的迹**

**Bk 为类别之间的协方差矩阵**

**Wk 为类别内部数据的协方差矩阵**

**m 为训练集样本数**

**k 为类别数**

![20211029165506](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029165506.png)

那什么是迹，代表什么？

矩阵的对角线（迹）可以表示一个物体的相似性

![20211029165801](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029165801.png)

CH需要达到的目的：

​用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。**分数s高则聚类效果越好。**

### 五.算法优化

#### 1.k-means算法小结

**优点：**

​1.原理简单（靠近中心点），实现容易

​2.聚类效果中上（依赖K的选择）

3.空间复杂度o(N)，时间复杂度o(IKN)

***N为样本点个数，K为中心点个数，I为迭代次数***

**缺点：**

​1.对离群点，噪声敏感 （中心点易偏移）

​2.很难发现大小差别很大的簇及进行增量计算

​3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）

#### 2.Canopy算法配合初始聚类

通过绘制同心圆，进行k值选择筛选，需要确定同心圆的半径t1、t2。

##### 2.1实现流程

![20211029172717](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211029172717.png)

##### 2.2优缺点

**优点：**

​1.Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的Cluster直接去掉有利于抗干扰。

​2.Canopy选择出来的每个Canopy的centerPoint作为K会更精确。

​3.只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。

**缺点：**

​1.算法中 T1、T2的确定问题 ，依旧可能落入局部最优解

#### 3.K-means++

对距离平方进行求解，保证下一个质心到当前质心，距离最远。kmeans++目的，是让选择的质心尽可能的分散。

![20211030100626](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030100626.png)

如果第一个质心选择在圆心，那么最优可能选择到的下一个点在`P(A)`这个区域

![20211030101338](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030101338.png)

#### 4.二分k-means

通过误差平方和，设置阈值，然后进行划分。

##### 4.1实现流程:

1.所有点作为一个簇

2.将该簇一分为二

3.选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。

4.以此进行下去，直到簇的数目等于用户给定的数目k为止。

![20211030103214](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030103214.png)
![20211030103230](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030103230.png)

##### 4.2隐含的一个原则

因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。

二分K均值算法可以加速K-means算法的执行速度，因为它的相似度计算少了并且不受初始化问题的影响，因为这里不存在随机点的选取，且每一步都保证了误差最小。

#### 5.k-medoids（k-中心聚类算法）

K-medoids和K-means是有区别的，**不一样的地方在于中心点的选取**

K-means中，将中心点取为当前cluster中所有数据点的平均值，对异常点很敏感!

K-medoids中，将从当前cluster 中选取到其他所有（当前cluster中的）点的距离之和最小的点作为中心点，即从当前点选择中心点（质心）进行判断。

![20211030104841](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030104841.png)

**算法流程：**

( 1 )总体n个样本点中任意选取k个点作为medoids

( 2 )按照与medoids最近的原则，将剩余的n-k个点分配到当前最佳的medoids代表的类中

( 3 )对于第i个类中除对应medoids点外的所有其他点，按顺序计算当其为新的medoids时，代价函数的值，遍历所有可能，选取代价函数最小时对应的点作为新的medoids

( 4 )重复2-3的过程，直到所有的medoids点不再发生变化或已达到设定的最大迭代次数

( 5 )产出最终确定的k个类

**k-medoids对噪声鲁棒性好。**

#### 6.Kernel k-means

kernel k-means实际上，就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。

![20211030104933](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030104933.png)

#### 7.ISODATA

**动态聚类，可以更改k值的大小。** 类别数目随着聚类过程而变化；

对类别数会进行合并，分裂，

“合并”：（当聚类结果某一类中样本数太少，或两个类间的距离太近时）

“分裂”（当聚类结果中某一类的类内方差太大，将该类进行分裂）

#### 8.Mini Batch K-Means

**适合大数据的聚类算法**

大数据量是什么量级？通常**当样本量大于1万做聚类时**，就需要考虑选用Mini Batch K-Means算法。

Mini Batch KMeans使用了Mini Batch（分批处理）的方法对数据点之间的距离进行计算。

Mini Batch计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本量少，所以会相应的减少运行时间，但另一方面抽样也必然会带来准确度的下降。

`该算法的迭代步骤有两步：`

(1)从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心

(2)更新质心

​ 与Kmeans相比，数据的更新在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。

#### 9.总结

![20211030105434](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211030105434.png)