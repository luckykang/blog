---
layout: post
title: 机器学习系列10丨NLP 基础文本分析模型
tag: 机器学习
---

### 一、基础文本分析模型之一：隐语义分析

#### 1.隐语义分析的背景

对于数据挖掘而言，文本数据算是大规模数据中，研究人员最早接触到的一类数据了。长久以来，大家都有一种直观的想法，那就是在这些看似没有头绪的文字中，究竟有没有隐含着某些规律呢？我们到底能不能从文字中提取出一些更加有用的结构性的内容呢？

**对于文本分析，有一类是基于“显式”的标签来进行的**。也就是说，我们可以把文本分析当作是**监督学习的任务**来看待。这一类文本分析的一大特点，往往是针对某一种任务建立分类器，然后对不同类别的文本进行鉴别，从而达到更加深入理解文本的目的。比如，我们需要理解不同情感的文字的时候，通常情况下，我们需要有一个数据集，能够告诉我们哪些文档 是“正面情绪”的，哪些是“负面情绪”的。

然而，并不是所有的文本分析任务都是建立在有数据标签的基础之上。实际上，对于绝大多数文本数据而言，我们事先是并没有标签信息的。那么，**在没有标签信息的场景下，如何对文本提取关键信息就成为了研究人员长期面对的一个关键挑战**。如果我们用今天的眼光来看，隐语义分析的核心其实就是用无监督的方法从文本中提取特性，而这些特性可能会对原文本的深层关系有着更好的解释。

#### 2.隐语义分析

对隐语义分析的一个简单直白的解释就是：**利用矩阵分解的概念对“词 - 文档矩阵”（Term-Document Matrix）进行分解**。

这里面的一个隐含的假设就是，**“词 - 文档矩阵”是一个稀疏矩阵**。什么意思？意思就是从大规模的文字信息来说，文字服从一个叫“**幂定律**”（Power Law Distribution）的规律。那就是绝大多数的单词仅出现很少的次数，而少数的单词会出现在很多文档中。我们也可以理解成一种变形的“20/80”原理，也就是 20% 的单词出现在 80% 的文档中。当然，文字的幂定理规则的一个直接结果就是“词 - 文档矩阵”是稀疏矩阵。这个矩阵里面有大量的零，代表很多单词都没有出现在那些文档中。

对一个稀疏矩阵，我们往往假设原有的矩阵并不能真正表示数据内部的信息。也就是说，我 们认为可能会有一个结构存在于这个矩阵之中。而这个假设，就是我们经常会在矩阵分解这 个语境中提到的“**低维假设**”（Low-rank Approximation）。你不必去担心这个低维假设 的本质意义，我们只需要理解这个低维假设的核心，就是我们**可以用比较少的维度来表达原 来的这个稀疏的矩阵**。

试想我们拥有一个 N 乘 M 的“词 - 文档矩阵”，也就是说我们有 N 个单词，M 个文档。 在这个稀疏矩阵的数据中，矩阵分解的基本思想是希望得到一个 N 乘以 K 的单词矩阵，以及一个 K 乘以 M 的文档矩阵。K 是一个事先指定好的参数，这也是矩阵分解的一个核心问题，那就是如何选择这个 K。我们可以看到，这种分解能够还原之前的 N 乘以 M 的“词文档矩阵”。

那么，这两个新的矩阵有什么“含义”呢？人们通过对很多数据的分解以后发现，单词矩阵往往能够把一些在某种语境下的单词给聚拢。比如我们会发现，很多和体育相关的词会聚拢在某个维度下，而很多和金融相关的词会聚拢在另外一个维度下。慢慢地，大家就开始把每一个维度认定为一个“主题”。那么，**基于矩阵分解的隐语义分析其实就是最早的主题模型**。而文档矩阵则描述了不同文档在我们 K 个主题下的强度。

值得注意的是，我们这里为了介绍隐语义模型的实际意义而隐藏了一些实际的技术细节。从历史上看，比较流行的隐语义模型其实是基于“**奇异值分解**”（Singular Value Decomposition），也就是我们常常听到的**SVD 分解**。由于篇幅有限，我们这里就不针对 SVD 分解展开讨论了。即便是 SVD 分解，其核心思想依然是我们刚才讲到的分解出来的主 题矩阵。

基于矩阵分解的隐语义模型也有其局限性，最大的一个问题就是分解出来的矩阵本身都是实 数，也就是有负数和正数，这也限制了我们真正用这些数来进行一些含义的推断。然而，即 便如此，**在很长的一段时间里，基于 SVD 的隐语义模型可以说是标准的无监督文本挖掘的核心算法**。

### 二、基础文本分析模型之二：概率隐语义分析

概率隐语义分析有时候又被简称为 **PLSA**（Probability Latent Semantic Analysis）。

既然概率隐语义分析是利用概率的语言，那么我们就来看看概率隐语义分析是如何对文档进行建模的。

首先，**PLSA 是对文档和里面单词的联合分布进行建模**。这个文档和单词的联合分布其实就是类似隐语义分析中的那个文档和单词的矩阵。只不过，在 PLSA 里，我们不是直接对数据 进行建模，而是认为数据是从某个分布中产生的结果。那么，对于这个联合分布该如何建模呢？

一种方法就是对这个联合分布**直接进行建模**，但是这往往会遇到数据不足的情况，或者无法找到一个合适的已知参数的分布来直接描述我们需要建模的这个联合分布。另外一种经常使 用的方法就是简化这个联合分布，从而找到我们可以建模的形式。

那么，如何简化联合分布呢？一种方法就是**把一个联合分布进行分解**。

**一种分解分布的方法就是假定一些隐含的变量，然后数据又是从这些隐含变量中产生而来**。 在我们现在的情况里，我们从文档和单词的联合分布入手，可以做出这样的假设：这个文档 和单词的联合分布是，我们首先从文档出发，产生了当前所需要的主题（比如金融、运动等），然后再从主题出发，产生相应的单词。很明显，这里的主题是我们并不知道的隐含变量，是需要我们从数据中估计出来的。这就是 PLSA 模型的基本假设。

PLSA 还有一个**等价的模型描述**，也是对文档单词联合分布的另外一种分解，那就是我们首先假设有一个主题的先验概率，然后根据这个主题的分布，产生了一个文档，同时也产生了这个文档里面的所有单词。这种假设观点非常类似我们之前在介绍高级的主题模型时谈到的“下游方法”（Down-Stream）。这里文档变量和单词变量都成为了隐变量，也就是主题变量的下游变量。通过一定的代数变形，我们可以得到这两种方法其实就是等价的。

如果我们按照第一种分解方法来认识文档单词分布，有一种更加通俗的解释：我们其实是给 一个单词都联系了一个未知的主题变量，这个主题变量是从一个文档级别的主题分布得来的，实际上，这是一个多项分布（Multinomial Distribution）；然后，根据这个主题变量，我们又从相应的一个语言模型中，抽取出了当前的单词，这又是另外的一个多项分布。如果从这个角度来看待这个模型，你会发现，**PLSA 其实和 LDA 非常相似**。

**实际上，从模型的根本特征上来说，PLSA 和 LDA 都是对文档单词分布的一种分解，或者叫作产生解释**。只不过，LDA 针对刚才我们所说的两个多项分布，一个是每个文档的主题分布，另外一个是 K 个语言模型，都外加了先验分布，使得整个模型更加符合贝叶斯统计 的观点。然而在**建模的核心思想上，PLSA 和 LDA 是一样的**。

### 三、基础文本分析模型之三：EM算法


#### 1.EM算法介绍

“概率隐语义分析”（Probabilistic Latent Semantic Indexing），或者简称为 PLSA，这类模型有效地弥补了隐语义分析的不足，在 LDA 兴起之前，成为了有力的文本分析工具。

不管是 PLSA，还是 LDA，其模型的训练过程都直接或者间接地依赖一个算法，这个算法叫作“**期望最大化**”（Expectation Maximization），或简称为 **EM 算法**。实际上，EM 算法是针对隐参数模型（Latent Variable Model）最直接有效的训练方法之一。

#### 2.EM 和 MLE 的关系

EM 算法深深根植于一种更加传统的统计参数方法：**最大似然估计**（Maximum Likelihood Estimation），有时候简称为 **MLE。绝大多数的机器学习都可以表达成为某种概率模型的 MLE 求解过程**。

具体来说，MLE 是这样构造的。首先，我们通过概率模型写出当前数据的“似然表达”。 所谓的“似然”表达，其实也就是在当前模型的参数值的情况下，看整个数据出现的可能性有多少。可能性越低，表明参数越无法解释当前的数据。反之，如果可能性非常高，则表明参数可以比较准确地解释当前的数据。因此，**MLE 的思想其实就是找到一组参数的取值，使其可以最好地解释现在的数据**。

针对某一个模型写出这个 MLE 以后，就是一个具体的式子，然后看我们能否找到这个式子 最大值下的参数取值。这个时候，整个问题往往就已经变成了一个优化问题。从优化的角度 来说，那就是针对参数求导，然后尝试把整个式子置零，从而求出在这个时候的参数值。

对绝大多数相对比较简单的模型来说，我们都可以根据这个流程求出参数的取值。比如，我 们熟悉的利用高斯分布来对数据进行建模，其实就可以通过 MLE 的形式，写出用高斯建模 的似然表达式，然后通过求解最优函数解的方式得到最佳的参数表达。而正好，这个最优的 参数就是样本的均值和样本的方差。

然而，并不是所有的 MLE 表达都能够得到一个“**解析解**”（Closed Form Solution），有不少的模型甚至无法优化 MLE 的表达式，那么这个时候，我们就需要一个新的工具来求解 MLE。

**EM 算法的提出就是为了简化那些求解相对比较困难模型的 MLE 解**。

有一点需要说明的是，EM 算法并不能直接求到 MLE，而只能提供一种近似。多数无法直接求解的 MLE 问题都属于**非凸（Non-Convex）问题**。因此，**EM 能够提供的仅仅是一个 局部的最优解，而不是全局的最优解**。

#### 3.EM算法的核心思想

理解了 EM 和 MLE 的关系后，我们来看一看 EM 的一些核心思想。因为 EM 算法是技术性 比较强的算法，我建议你一定要亲自去推演公式，从而能够真正理解算法的精髓。我们在这 里主要提供一种大体的思路。

EM 算法的一种解释是这样的。首先，我们可以通过代数变形，为每一个数据点的似然公式 找到一个新的概率分布，而这个概率分布是通过一个隐含变量来达到的。很明显，在理论 上，我们可以通过把这个隐含变量积分掉来达到恢复原始的 MLE 公式的目的。

然而，这里遇到的一个大的阻碍就是，在 MLE 公式里面，有一个求对数函数（log）在这个积分符号外面。这就导致整个式子无法进行操作。通俗地讲，EM 就是要针对这样的情况，试图把这个在积分符号之外的求对数函数拿到积分符号里面。能够这么做，是因为有一个不等式，叫“**杨森不等式**”。你不需要去理解杨森不等式的细节，大体上这个不等式是 说，函数的期望值要大于或等于先对函数的变量求期望然后再对其作用函数。

于是，在这样的一个不等式的引领下，我们刚才所说的积分，其实就可以被看作是对某一个 函数求期望值。而这个函数，恰好就是模型的似然表达。通过杨森不等式，我们可以把对数 函数拿到积分符号里面，这样当然就无法保持等号了，也就是说，这一步的操作不是一个等 值操作。利用杨森不等式之后的式子其实是原来的式子，也就是含有隐含变量的 MLE 式的一个“**下限**”（Lower Bound）。

**利用杨森不等式，从而写出一个原始的 MLE 的下限，是标准的 EM 算法以及一系列基于变分 EM（Variational EM）算法的核心思想。这么做的目的其实就是把对数函数从积分的外面给拿到里面**。

当我们有了这个下限之后，我们就可以套用 MLE 的一切流程了。注意，这时候，我们有两组未知数。一组未知数是我们**模型的参数**，另外一组未知数就是**模型的隐含变量**。于是，当 得到下限之后，我们就需要对这两组未知数分别求导，并且得到他们的最优表达。

当我们按照当前的模型参数，对模型的隐含变量所对应的概率分布求解后，最优的隐含变量 的概率分布就等于隐含变量基于数据的后验概率。什么意思呢？意思就是说，如果我们把隐 含变量的取值直接等于其后验概率分布，就得到了当前的最优解。这个步骤常常被叫作“**E 步**”。

在进行了 E 步之后，我们再按照当前的隐含变量，求解这个时候最佳的模型参数。这常常被认为是“**M 步”。一次 E 步，一次 M 步则被认为是 EM 算法的一个迭代轮回**。

EM 算法貌似很神秘，但如果我们理解了整个流程的精髓，就可以把这个算法总结为：EM 算法是利用**杨森不等式得到 MLE 的一个下限，并且优化求解模型参数和模型的隐含变量的 一个过程**。

掌握了这个精髓，我们就可以看到，为什么 LDA 和 PLSA 等隐变量模型需要利用 EM 或者 类似 EM 的步骤进行求解。第一，这些模型的 MLE 都有一个对数函数在积分符号外面，使 得这个过程无法直接求解。第二，这些模型本身就有隐含变量，因此不需要额外制造新的隐含变量。