---
layout: post
title: 机器学习系列13丨线性回归
tag: 机器学习
---

### 一.线性回归（Linear regression）概述

#### 1.定义

是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式。

#### 2.特点

只有一个自变量的情况称为单变量回归，多余一个自变量的情况叫做多元回归。

#### 3.公式

![20211014145606](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014145606.png)

#### 4.分类：

- 线性关系
  - 单变量线性关系
  - 多变量线性关系
- 非线性关系

### 二.线性回归API初步使用

#### 1.线性回归api

![20211014151223](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014151223.png)

#### 2.代码演示

    # 导入模块
    from sklearn.linear_model import LinearRegression

    # 构造数据集
    x = [[80, 86], [82, 80], [85, 78], [90, 90], [86, 82], [82, 90], [78, 80], [92, 94]]
    y = [84.2, 80.6, 80.1, 90, 83.2, 87.6, 79.4, 93.4]

    # 模型训练
    # 实例化API
    estimator = LinearRegression()
    # 使用fit方法进行训练
    estimator.fit(x, y)
    # 查看系数值
    coef = estimator.coef_
    print("系数值：", coef)  # 系数值： [0.3 0.7]

    # 预测
    print("预测值是：", estimator.predict([[90, 90]]))  # 预测值是： [90.]

### 三.线性回归的损失和优化

#### 1.损失函数

又称最小二乘法

![20211014154730](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014154730.png)

#### 2.线性回归常用的优化算法

- **正规方程**
- **梯度下降法**

### 四.正规方程

#### 1.介绍

![20211014155012](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014155012.png)

#### 2.正规方程的推导

利用矩阵的逆，转置进行一步求解，只适合样本和特征比较少的情况。

- 推导方式1：

![20211014164543](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014164543.png)
![20211014165302](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014165302.png)

- 推导方式2：

![20211014165528](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014165528.png)

### 五.梯度下降

#### 1.概念

梯度下降的基本过程就和下山的场景类似。

首先，我们有一个 **可微分的函数**，这个函数就代表着一座山。

我们的目标就是找到 **这个函数的最小值**，也就是山底。

最快的下山方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走。对应到函数中，就是 **找到给定点的梯度**。然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数值变化最快的方向。所以我们反复利用这个方法，反复求取梯度，最后就能到达局部的最小值。

在`单变量的函数`中，梯度其实就是 **函数的微分**，代表着函数在某个给定点的切线的斜率。

在`多变量的函数`中，梯度是一个向量，向量有方向，**梯度的方向就指出了给定点的上升最快的方向**。

![20211014170505](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014170505.png)

#### 2.单变量函数的梯度下降

![20211014174430](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211014174430.png)

