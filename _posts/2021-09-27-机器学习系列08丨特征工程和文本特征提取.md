---
layout: post
title: 机器学习系列08丨特征工程和文本特征提取
tag: 机器学习
---

### 二.特征工程

#### 1.常用数据集数据的结构组成

结构：特征值+目标值

**注意**：有些数据集没有目标值

#### 2.数据中对于特征的处理

pandas:一个数据读取非常方便以及基本的处理格式的工具

sklearn:对于特征的处理提供了强大的接口

#### 3.特征工程定义及意义

定义：是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的预测准确性。

意义：直接影响预测结果

#### 4.Scikit-learn库介绍

- python语言的机器学习工具
- Scikit-learn包括许多知名的机器学习算法的实现
- Scikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎

安装Scikit-learn

    pip install Scikit-learn

导入
    
    import sklearn

**注意** 安装scikit-learn需要Numpy、pandas等库

#### 5.特征抽取-字典特征抽取

**特征抽取**:对文本等数据进行特征值化

**字典数据抽取**：把字典中的一些类别数据，分别进行转换成特征

**字典特征抽取作用**：对字典数据进行特征值化

类：`sklearn.feature_extraction.DictVectorizer`

##### DicVectorizer语法

![20210928152013](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210928152013.png)

初始代码：

    from sklearn.feature_extraction import DictVectorizer

    def dictvec():
        # 实例化
        dict = DictVectorizer()
        # 调用fit_transform
        data = dict.fit_transform([{'city':'北京','temperature':100},{'city':'上海','temperature':60},{'city':'深圳','temperature':30}])
        print(data)


    if __name__ == "__main__":
        dictvec()

结果如下：

![20210929165905](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210929165905.png)

转换成One-hot编码：

    from sklearn.feature_extraction import DictVectorizer

    def dictvec():
        # 实例化
        dict = DictVectorizer(sparse=False)
        # 调用fit_transform
        data = dict.fit_transform([{'city':'北京','temperature':100},{'city':'上海','temperature':60},{'city':'深圳','temperature':30}])
        # 显示特征值
        print(dict.get_feature_names())
        # 转换特征数据
        print(dict.inverse_transform(data))
        print(data)


    if __name__ == "__main__":
        dictvec()

结果：

![20210929172342](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210929172342.png)

#### 6.特征抽取-文本特征抽取

**文本特征抽取作用** ：对文本数据进行特征值化

##### 6.1 CountVectorizer方法

类：`sklearn.feature_extraction.text.CountVectorizer`

**CountVectorizer语法**

![20210929170529](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210929170529.png)

**流程**

1.实例化类CountVectorizer

2.调用fit_transform方法输入数据并转换

注意返回格式，利用toarray()进行sparse矩阵转换array数组

示例代码：

    from sklearn.feature_extraction.text import CountVectorizer

    def countvec():
        cv = CountVectorizer()
        data = cv.fit_transform(["life is short,i like python","life is long,i dislike java"])
        # 统计所有句子中的所有的词，重复的词只算一次 [词的列表]
        print(cv.get_feature_names())
        print(data.toarray())


    if __name__ == "__main__":
        countvec()

结果如下：

![20210929174218](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210929174218.png)

我们可以看到，对于单个英文字母是没有统计的，理由是没有分类依据，单个汉字也一样。

![20210929175516](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210929175516.png)

英文默认有空格，所以不需要分词，中文可以使用`jieba`来分词。

![20210929180232](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210929180232.png)

代码实现：

    from sklearn.feature_extraction.text import CountVectorizer
    import jieba

    def cutword():
        con1 = jieba.cut("我们可以看到，对于单个英文字母是没有统计的，理由是没有分类依据，单个汉字也一样。")
        con2 = jieba.cut("英文默认有空格，所以不需要分词，中文可以使来分词。")
        # 转换成列表
        contest1 = list(con1)
        contest2 = list(con2)

        # 转换成字符串
        c1 = ' '.join(contest1)
        c2 = ' '.join(contest2)
        print(c1, c2)
        return c1,c2


    def countvec():
        c1, c2 = cutword()
        cv = CountVectorizer()
        data = cv.fit_transform([c1, c2])
        # 统计所有文章中的所有的词
        print(cv.get_feature_names())
        print(data.toarray())

        return None

    if __name__ == "__main__":
        countvec()

结果如下：

![20210930103417](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210930103417.png)

对于两篇文章出现的很多中性词，会对特征的抽取造成影响，这时候我们可以使用朴素贝叶斯的方法来处理。

##### 6.2 TF-IDF方法

|                             |       |
|            ----             | ----  |
| tf(term frequency):词的频率  | 出现的次数 |
| idf(inverse document frequency):逆文档频率  | log(总文档数量/该词出现的文档数量) |

`log(数值)`：输入的数值越小，结果越小

`tf * idf` ：重要性程度

**TF-IDF主要思想是** ：如果某个词或短语在一篇文章中出现的概率高，并在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

**TF-IDF作用** ：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

类：sklearn.feature_extraction.text.TfidfVectorizer

TfidfVectorizer语法：

![20210930134801](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210930134801.png)

代码实现：

    from sklearn.feature_extraction.text import TfidfVectorizer
    import jieba

    def cutword():
        con1 = jieba.cut("在模型优化器中添加对自定义操作的支持，以便模型优化器可以通过操作生成 IR")
        con2 = jieba.cut("创建一个操作集，并且在描述它实现自定义nGraph操作自定义nGraph操作")
        # 转换成列表
        contest1 = list(con1)
        contest2 = list(con2)

        # 转换成字符串
        c1 = ' '.join(contest1)
        c2 = ' '.join(contest2)
        print(c1, c2)
        return c1,c2


    def tfidfvec():
        c1, c2 = cutword()
        tf = TfidfVectorizer()
        data = tf.fit_transform([c1, c2])
        # 统计所有句子中的所有的词，重复的词只算一次，单个字母不统计  [词的列表]
        print(tf.get_feature_names())
        print(data.toarray())

        return None

    if __name__ == "__main__":
        tfidfvec()

结果：

![20210930140049](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20210930140049.png)





