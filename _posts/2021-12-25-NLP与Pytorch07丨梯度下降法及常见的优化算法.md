---
layout: post
title: NLP与Pytorch07丨梯度下降法及常见的优化算法
tag: NLP
---

## 一. 梯度下降算法（batch gradient descent BGD）

每次迭代都需要把**所有样本都送入**，这样的好处是每次迭代都顾及了全部的样本，做的是**全局最优化,但是有可能达到局部最优。**

## 二. 随机梯度下降法 (Stochastic gradient descent SGD)

针对梯度下降算法训练速度过慢的缺点，提出了随机梯度下降算法，随机梯度下降算法算法是从样本中**随机抽出一组**，训练后按**梯度更新**一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。

torch中的api为：`torch.optim.SGD()`

## 三. 小批量梯度下降 (Mini-batch gradient descent MBGD）

SGD相对来说要快很多，但是也有存在问题，由于单个样本的训练可能会带来很多噪声，使得SGD并不是每次迭代都向着整体最优化方向，因此在刚开始训练时可能收敛得很快，但是训练一段时间后就会变得很慢。在此基础上又提出了小批量梯度下降法，它是每次从样本中**随机抽取一小批进行训练，而不是一组**，这样即保证了效果又保证的速度。

## 四. 动量法

mini-batch SGD算法虽然这种算法能够带来很好的训练速度，但是在到达最优点的时候并不能够总是真正到达最优点，而是在最优点附近徘徊。

另一个缺点就是mini-batch SGD需要我们挑选一个合适的学习率，当我们采用小的学习率的时候，会导致网络在训练的时候收敛太慢；当我们采用大的学习率的时候，会导致在训练过程中优化的幅度跳过函数的范围，也就是可能跳过最优点。我们所希望的仅仅是网络在优化的时候网络的损失函数有一个很好的收敛速度同时又不至于摆动幅度太大。

所以**Momentum优化器**刚好可以解决我们所面临的问题，它主要是**基于梯度的移动指数加权平均，对网络的梯度进行平滑处理的，让梯度的摆动幅度变得更小**。

$$
\begin{align*}
&gradent = 0.8\nabla w + 0.2 history\_gradent  &，\nabla w 表示当前一次的梯度\\
&w = w - \alpha* gradent &，\alpha表示学习率
\end{align*}
$$

（注：t+1的的histroy_gradent 为第t次的gradent）

## 五. AdaGrad 

AdaGrad算法就是**将每一个参数的每一次迭代的梯度取平方累加后再开方，用全局学习率除以这个数**，作为学习率的动态更新，从而达到**自适应学习率**的效果
$$
\begin{align*}
&gradent = history\_gradent + (\nabla w)^2 \\
&w = w - \frac{\alpha}{\sqrt{gradent}+\delta} \nabla w ,&\delta为小常数，为了数值稳定大约设置为10^{-7}
\end{align*}
$$

## 六. RMSProp

Momentum优化算法中，虽然初步解决了优化中摆动幅度大的问题,为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对**参数的梯度使用了平方加权平均数**。 
$$
\begin{align*}
& gradent = 0.8*history\_gradent + 0.2*(\nabla w)^2 \\
& w = w - \frac{\alpha}{\sqrt{gradent}+\delta} \nabla w
\end{align*}
$$

## 七. Adam

Adam（Adaptive Moment Estimation）算法是将Momentum算法和RMSProp算法结合起来使用的一种算法,**能够达到防止梯度的摆幅多大，同时还能够加快收敛速度**

**就目前而言，Adam是整体最好的选择。**

$$
\begin{align*}
& 1. 需要初始化梯度的累积量和平方累积量 \\
& v_w = 0,s_w = 0 \\
& 2. 第 t 轮训练中，我们首先可以计算得到Momentum和RMSProp的参数更新：\\
& v_w = 0.8v  + 0.2 \nabla w \qquad,Momentum计算的梯度\\
& s_w = 0.8*s + 0.2*(\nabla w)^2 \qquad,RMSProp计算的梯度\\
& 3. 对其中的值进行处理后，得到：\\
& w = w - \frac{\alpha}{\sqrt{s_w}+\delta} v_w
\end{align*}
$$
torch中的api为：`torch.optim.Adam()`


## 效果展示：

![20211228140432](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/20211228140432.png)